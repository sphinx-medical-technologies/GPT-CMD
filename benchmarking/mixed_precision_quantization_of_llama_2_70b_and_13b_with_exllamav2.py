# -*- coding: utf-8 -*-
"""Mixed-Precision Quantization of Llama 2 70B and 13B with ExLlamaV2

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18nos7aDanG1s-s1tfjqtWz40ug2bw38f

This notebook shows how to quantize and run LLMs with mixed precision using ExLlamaV2.

More details in this article: [Run Llama 2 70B on Your GPU with ExLlamaV2](https://kaitchup.substack.com/p/run-llama-2-70b-on-your-gpu-with)

Note that when I wrote this notebook, ExLlamaV2 was still a very young project. If you find that it doesn't work anymore, please leave a comment in the article above. I'll update the notebook.
"""

# Commented out IPython magic to ensure Python compatibility.
!pip install transformers
!git clone https://github.com/turboderp/exllamav2
# %cd exllamav2
!pip install -r requirements.txt

"""We will download Llama 2 from the Hugging Face Hub. We must login."""

from huggingface_hub import login
login(
  token="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",
  add_to_git_credential=True
)

"""ExLlamaV2 doesn't communicate well with Hugging Face transformers. You first need to download the models locally. We only need the safetensors version. ExLlamaV2 uses safetensors so we don't need to download the ".bin" files."""

#The directory where we want to store the model must exist.
!mkdir ./Llama-2-13b-hf/

from huggingface_hub import snapshot_download
snapshot_download(repo_id="meta-llama/Llama-2-13b-hf", ignore_patterns=["*.bin"], local_dir="./Llama-2-13b-hf/", local_dir_use_symlinks=False)

"""We need a dataset for calibrating the quantization. I use wikitext test set. Again, we need it locally so I download the .parquet file."""

!wget https://huggingface.co/datasets/wikitext/resolve/refs%2Fconvert%2Fparquet/wikitext-2-v1/test/0000.parquet

"""The quantization of Llama 2 13B is done with convert.py. It takes around 2 hours."""

!mkdir ./Llama-2-13b-hf/temp/
!python convert.py \
    -i ./ \
    -o ./Llama-2-13b-hf/temp/ \
    -c 0000.parquet \
    -cf ./Llama-2-13b-hf/3.0bpw/ \
    -b 3.0

"""You can use test_inference.py to try the model. In the remainder of this notebook, for the demonstration I use Llama 2 70B with an average 2.55 bpw. It was created by ExLlamaV2 authors."""

!pip install transformers

!mkdir ./Llama-2-70b-2.5bpw/

from huggingface_hub import snapshot_download

snapshot_download(repo_id="turboderp/Llama2-70B-exl2", ignore_patterns=["*.bin"], revision="2.5bpw", local_dir="./Llama-2-70b-2.5bpw/", local_dir_use_symlinks=False)

!python test_inference.py -m ./Llama-2-70b-2.5bpw/ -p "Once upon a time,"

!nvidia-smi